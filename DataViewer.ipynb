{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.443 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.1 seconds.\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from nuscenes.nuscenes import NuScenes\n",
    "import nuscenes.lidarseg\n",
    "from nuscenes.utils.data_classes import LidarPointCloud, RadarPointCloud, Box\n",
    "import utils_data_viewer\n",
    "from utils_data_viewer import view_scene_map, custom_draw_geometry, cam_vis, top_down, interactive_vis, visualize_lidar_in_img\n",
    "from utils_data_viewer import project_points_in_img, colorise_pcd\n",
    "#Radar\n",
    "from utils_data_viewer import top_down_radar, interactive_vis_radar, radar_from_file, fuse_radars_in_ego, visualize_radar_in_img\n",
    "import utils_obj_det\n",
    "from utils_obj_det import get_radar_in_img, vis_obj_det_speed\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from scipy.spatial.transform import Rotation\n",
    "from pyquaternion import Quaternion\n",
    "import struct\n",
    "import matplotlib as plt\n",
    "\n",
    "import cv2\n",
    "import open3d as o3d\n",
    "\n",
    "\n",
    "nusc = NuScenes(version='v1.0-mini', dataroot='/data/sets/nuscenes', verbose=True)\n",
    "utils_data_viewer.nusc = nusc\n",
    "utils_obj_det.nusc = nusc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nusc.list_scenes()\n",
    "sc_id = 9\n",
    "\n",
    "scene = nusc.scene[sc_id]\n",
    "log = nusc.get('log', scene['log_token'])\n",
    "map = nusc.get('map', log['map_token'])\n",
    "map_file = os.path.join(nusc.dataroot, map['filename'])\n",
    "\n",
    "first_sample = nusc.get('sample', scene['first_sample_token'])\n",
    "last_sample = nusc.get('sample', scene['last_sample_token'])\n",
    "sample = first_sample\n",
    "cam_vis(scene, \"CAM_FRONT\", record=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_ids = [0, 2, 3, 4, 6, 7, 8]\n",
    "channels = ['CAM_FRONT', 'CAM_FRONT_LEFT', 'CAM_FRONT_RIGHT', 'CAM_BACK', 'CAM_BACK_LEFT', 'CAM_BACK_RIGHT']\n",
    "for sc_i in sc_ids:\n",
    "    scene = nusc.scene[sc_i]\n",
    "    # for c in channels:\n",
    "    #     cam_vis(scene, c, record=True)\n",
    "    #     visualize_lidar_in_img(scene, c, record=True)\n",
    "    #     visualize_radar_in_img(scene, c, record=True)\n",
    "    #     vis_obj_det_speed(scene, cam_channel=c, record=True)\n",
    "    # top_down(scene, record=True)\n",
    "    #top_down_radar(scene, record=True)\n",
    "    interactive_vis(scene, record=True)\n",
    "    colorise_pcd(scene, record=True)\n",
    "    interactive_vis_radar(scene, record=True)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RADAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nusc.render_pointcloud_in_image(sample['token'], pointsensor_channel='LIDAR_TOP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lidar\n",
    "#view_scene_map(scene)\n",
    "#visualize_lidar_in_img(scene, cam_channel='CAM_FRONT', record=True)\n",
    "#colorise_pcd(scene, record=True)\n",
    "cam_vis(scene, \"CAM_FRONT\")\n",
    "#interactive_vis(scene, record=True)\n",
    "#top_down(scene, record=True)\n",
    "\n",
    "#Radar\n",
    "#interactive_vis_radar(scene, record=True)\n",
    "#top_down_radar(scene, record=True)\n",
    "#visualize_radar_in_img(scene, cam_channel='CAM_BACK', record=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BACKUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radar_channel = 'RADAR_FRONT'\n",
    "radar = nusc.get('sample_data', sample['data'][radar_channel])\n",
    "radar_file = os.path.join(nusc.dataroot, radar['filename'])\n",
    "pts, fields = radar_from_file(radar_file)\n",
    "radar_sensor = nusc.get('calibrated_sensor', radar['calibrated_sensor_token'])\n",
    "velocities = pts[8:10, :]  # Compensated velocity\n",
    "\n",
    "s, f = fuse_radars_in_ego(sample)\n",
    "s.shape\n",
    "\n",
    "velocities = np.vstack((velocities, np.zeros(pts.shape[1])))\n",
    "pts[:3, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is basically just a copy from the radar pointcloud code from nuscenes devkit\n",
    "def radar_from_file(radar_file):\n",
    "\n",
    "    \"\"\"\n",
    "    Loads RADAR data from a Point Cloud Data file. See details below.\n",
    "    :param file_name: The path of the pointcloud file.\n",
    "    :param invalid_states: Radar states to be kept. See details below.\n",
    "    :param dynprop_states: Radar states to be kept. Use [0, 2, 6] for moving objects only. See details below.\n",
    "    :param ambig_states: Radar states to be kept. See details below.\n",
    "    To keep all radar returns, set each state filter to range(18).\n",
    "    :return: <np.float: d, n>. Point cloud matrix with d dimensions and n points.\n",
    "\n",
    "    Example of the header fields:\n",
    "    # .PCD v0.7 - Point Cloud Data file format\n",
    "    VERSION 0.7\n",
    "    FIELDS x y z dyn_prop id rcs vx vy vx_comp vy_comp is_quality_valid ambig_state x_rms y_rms invalid_state pdh0 vx_rms vy_rms\n",
    "    SIZE 4 4 4 1 2 4 4 4 4 4 1 1 1 1 1 1 1 1\n",
    "    TYPE F F F I I F F F F F I I I I I I I I\n",
    "    COUNT 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
    "    WIDTH 125\n",
    "    HEIGHT 1\n",
    "    VIEWPOINT 0 0 0 1 0 0 0\n",
    "    POINTS 125\n",
    "    DATA binary\n",
    "\n",
    "    Below some of the fields are explained in more detail:\n",
    "\n",
    "    x is front, y is left\n",
    "\n",
    "    vx, vy are the velocities in m/s.\n",
    "    vx_comp, vy_comp are the velocities in m/s compensated by the ego motion.\n",
    "    We recommend using the compensated velocities.\n",
    "\n",
    "    invalid_state: state of Cluster validity state.\n",
    "    (Invalid states)\n",
    "    0x01\tinvalid due to low RCS\n",
    "    0x02\tinvalid due to near-field artefact\n",
    "    0x03\tinvalid far range cluster because not confirmed in near range\n",
    "    0x05\treserved\n",
    "    0x06\tinvalid cluster due to high mirror probability\n",
    "    0x07\tInvalid cluster because outside sensor field of view\n",
    "    0x0d\treserved\n",
    "    0x0e\tinvalid cluster because it is a harmonics\n",
    "    (Valid states)\n",
    "    0x00\tvalid\n",
    "    0x04\tvalid cluster with low RCS\n",
    "    0x08\tvalid cluster with azimuth correction due to elevation\n",
    "    0x09\tvalid cluster with high child probability\n",
    "    0x0a\tvalid cluster with high probability of being a 50 deg artefact\n",
    "    0x0b\tvalid cluster but no local maximum\n",
    "    0x0c\tvalid cluster with high artefact probability\n",
    "    0x0f\tvalid cluster with above 95m in near range\n",
    "    0x10\tvalid cluster with high multi-target probability\n",
    "    0x11\tvalid cluster with suspicious angle\n",
    "\n",
    "    dynProp: Dynamic property of cluster to indicate if is moving or not.\n",
    "    0: moving\n",
    "    1: stationary\n",
    "    2: oncoming\n",
    "    3: stationary candidate\n",
    "    4: unknown\n",
    "    5: crossing stationary\n",
    "    6: crossing moving\n",
    "    7: stopped\n",
    "\n",
    "    ambig_state: State of Doppler (radial velocity) ambiguity solution.\n",
    "    0: invalid\n",
    "    1: ambiguous\n",
    "    2: staggered ramp\n",
    "    3: unambiguous\n",
    "    4: stationary candidates\n",
    "\n",
    "    pdh0: False alarm probability of cluster (i.e. probability of being an artefact caused by multipath or similar).\n",
    "    0: invalid\n",
    "    1: <25%\n",
    "    2: 50%\n",
    "    3: 75%\n",
    "    4: 90%\n",
    "    5: 99%\n",
    "    6: 99.9%\n",
    "    7: <=100%\n",
    "    \"\"\"\n",
    "    #RADAR - Read pcd from nuscenes devkit code\n",
    "    meta = []\n",
    "    with open(radar_file, 'rb') as f:\n",
    "        for line in f:\n",
    "            line = line.strip().decode('utf-8')\n",
    "            meta.append(line)\n",
    "            if line.startswith('DATA'):\n",
    "                break\n",
    "\n",
    "        data_binary = f.read()\n",
    "\n",
    "     # Get the header rows and check if they appear as expected.\n",
    "    assert meta[0].startswith('#'), 'First line must be comment'\n",
    "    assert meta[1].startswith('VERSION'), 'Second line must be VERSION'\n",
    "    #Get the fields of the data\n",
    "    fields = meta[2].split(' ')[1:]\n",
    "    sizes = meta[3].split(' ')[1:]\n",
    "    types = meta[4].split(' ')[1:]\n",
    "    counts = meta[5].split(' ')[1:]\n",
    "    width = int(meta[6].split(' ')[1])\n",
    "    height = int(meta[7].split(' ')[1])\n",
    "    data = meta[10].split(' ')[1]\n",
    "    feature_count = len(types)\n",
    "    assert width > 0\n",
    "    assert len([c for c in counts if c != c]) == 0, 'Error: COUNT not supported!'\n",
    "    assert height == 1, 'Error: height != 0 not supported!'\n",
    "    assert data == 'binary'\n",
    "\n",
    "    types# Lookup table for how to decode the binaries.\n",
    "    unpacking_lut = {'F': {2: 'e', 4: 'f', 8: 'd'},\n",
    "                        'I': {1: 'b', 2: 'h', 4: 'i', 8: 'q'},\n",
    "                        'U': {1: 'B', 2: 'H', 4: 'I', 8: 'Q'}}\n",
    "    types_str = ''.join([unpacking_lut[t][int(s)] for t, s in zip(types, sizes)])\n",
    "\n",
    "    # Decode each point.\n",
    "    offset = 0\n",
    "    point_count = width\n",
    "    points = []\n",
    "    for i in range(point_count):\n",
    "        point = []\n",
    "        for p in range(feature_count):\n",
    "            start_p = offset\n",
    "            end_p = start_p + int(sizes[p])\n",
    "            assert end_p < len(data_binary)\n",
    "            point_p = struct.unpack(types_str[p], data_binary[start_p:end_p])[0]\n",
    "            point.append(point_p)\n",
    "            offset = end_p\n",
    "        points.append(point)\n",
    "\n",
    "    # A NaN in the first point indicates an empty pointcloud.\n",
    "    point = np.array(points[0])\n",
    "    if np.any(np.isnan(point)):\n",
    "        return np.zeros((feature_count, 0))\n",
    "\n",
    "    # Convert to numpy matrix.\n",
    "    points = np.array(points).transpose()\n",
    "\n",
    "    # Class-level settings for radar pointclouds, see from_file().\n",
    "    invalid_states = [0]  # type: List[int]\n",
    "    dynprop_states = range(7)  # type: List[int] # Use [0, 2, 6] for moving objects only.\n",
    "    ambig_states = [3]  # type: List[int]\n",
    "\n",
    "     # If no parameters are provided, use default settings.\n",
    "    invalid_states = p_invalid_states if invalid_states is None else invalid_states\n",
    "    dynprop_states = p_dynprop_states if dynprop_states is None else dynprop_states\n",
    "    ambig_states = p_ambig_states if ambig_states is None else ambig_states\n",
    "\n",
    "    # Filter points with an invalid state. - 0 is an invalid state\n",
    "    valid = [p in invalid_states for p in points[-4, :]]\n",
    "    points = points[:, valid]\n",
    "\n",
    "    # Filter by dynProp. - 0 - 6 are moving, 7 is stopped\n",
    "    valid = [p in dynprop_states for p in points[3, :]]\n",
    "    points = points[:, valid]\n",
    "\n",
    "    # Filter by ambig_state. - Make sure Doppler ambiguity is unambiguous\n",
    "    valid = [p in ambig_states for p in points[11, :]]\n",
    "    points = points[:, valid]\n",
    "\n",
    "\n",
    "\n",
    "    return points, fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns np array of all radar points in ego frame\n",
    "def fuse_radars_in_ego(sample):\n",
    "    channels = ['RADAR_FRONT', 'RADAR_FRONT_LEFT', 'RADAR_FRONT_RIGHT', 'RADAR_BACK_LEFT', 'RADAR_BACK_RIGHT']\n",
    "    #Merged points in ego frame\n",
    "    radar_data =np.empty((18, 0))\n",
    "\n",
    "    for channel in channels:\n",
    "        #Load Radar in its own frame\n",
    "        radar = nusc.get('sample_data', sample['data'][channel])\n",
    "        radar_file = nusc.get_sample_data_path(radar['token'])\n",
    "        scan, fields = radar_from_file(radar_file) #scan 18 x N\n",
    "        # Transform radar points into coordinate frame of ego\n",
    "        #Get Lidar Pose\n",
    "        radar_sensor = nusc.get('calibrated_sensor', radar['calibrated_sensor_token'])\n",
    "        radar_t = np.asarray(radar_sensor['translation'])\n",
    "        radar_r_quat = Quaternion(radar_sensor['rotation'])\n",
    "\n",
    "        #Rotate and translate\n",
    "        scan[:3, :] = np.dot(radar_r_quat.rotation_matrix, scan[:3, :])\n",
    "        scan[:3, :] = (scan[:3, :].transpose() + radar_t).transpose()\n",
    "\n",
    "        radar_data = np.append(radar_data, scan, axis=1)\n",
    "    return radar_data, fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show video of selected camera with lidar rendered into the image\n",
    "def visualize_radar_in_img(scene, cam_channel='CAM_FRONT'):\n",
    "    first_sample = nusc.get('sample', scene['first_sample_token'])\n",
    "    last_sample = nusc.get('sample', scene['last_sample_token'])\n",
    "\n",
    "    winname = cam_channel\n",
    "    window_width = nusc.get('sample_data', first_sample['data'][cam_channel])['width']\n",
    "    window_height = nusc.get('sample_data', first_sample['data'][cam_channel])['height']\n",
    "    cv2.namedWindow(winname, cv2.WINDOW_NORMAL)\n",
    "    cv2.resizeWindow(winname, window_width, window_height)\n",
    "\n",
    "    #Load lidar and img data from scan\n",
    "    sample = first_sample\n",
    "\n",
    "    while True:\n",
    "        if sample['token'] == last_sample['token']:\n",
    "            sample = first_sample\n",
    "        #Load camera image\n",
    "        cam = nusc.get('sample_data', sample['data'][cam_channel])\n",
    "        cam_file = os.path.join(nusc.dataroot, cam['filename'])\n",
    "        cam_img = cv2.imread(cam_file)\n",
    "\n",
    "        scan, fields = fuse_radars_in_ego(sample) #scan 18 x N\n",
    "        points = scan.transpose()[:, :3] #Does not need to be converted to int - Needs to be Nx18 for o3d\n",
    "\n",
    "        #Get Camera Pose and intrinsic parameters\n",
    "        cam_sensor = nusc.get('calibrated_sensor', cam['calibrated_sensor_token'])\n",
    "        cam_t = np.asarray(cam_sensor['translation'])\n",
    "        cam_r_quat = Quaternion(cam_sensor['rotation'])\n",
    "        cam_k = np.asarray(cam_sensor['camera_intrinsic'])\n",
    "\n",
    "        #Transform points to image frame - Using Open3D transformations\n",
    "        pcd = o3d.geometry.PointCloud()\n",
    "        pcd.points = o3d.utility.Vector3dVector(points)\n",
    "\n",
    "        # #First transformation - Skip because Radar points are alread in ego frame\n",
    "        # pcd.rotate(radar_r_quat.rotation_matrix, np.array([0, 0, 0]))\n",
    "        # pcd.translate(radar_t)\n",
    "\n",
    "        #Second transformation - From ego to camera frame\n",
    "        pcd.translate(-cam_t)\n",
    "        pcd.rotate(cam_r_quat.rotation_matrix.T, np.array([0, 0, 0]))\n",
    "\n",
    "        #Project points into image plane\n",
    "        view = np.copy(cam_k)\n",
    "        points = np.asarray(pcd.points).T[:3, :]\n",
    "\n",
    "        #Set color to depth value\n",
    "        depth = points[2, :]\n",
    "\n",
    "        #Prepare intrinsics matrix for homogenous transformation\n",
    "        viewpad = np.eye(4)\n",
    "        viewpad[:view.shape[0], :view.shape[1]] = view\n",
    "\n",
    "        #Prepare points for homogenous transformation\n",
    "        points = np.concatenate((points, np.ones((1, points.shape[1])))) #4 X N matrix - Homogenous\n",
    "\n",
    "        #Project Points into image\n",
    "        points = np.dot(viewpad, points)\n",
    "\n",
    "        #Remove w\n",
    "        points = points[:3, :]\n",
    "\n",
    "        #Normalize along Z axis - Divide by depth\n",
    "        points = points / points[2, :]\n",
    "\n",
    "        # Remove points that are either outside or behind the camera. Leave a margin of 1 pixel for aesthetic reasons.\n",
    "        # Also make sure points are at least 1m in front of the camera to avoid seeing the lidar points on the camera\n",
    "        # casing for non-keyframes which are slightly out of sync.\n",
    "        min_dist = 1.0\n",
    "        mask = np.ones(depth.shape[0], dtype=bool)\n",
    "        mask = np.logical_and(mask, depth > min_dist)\n",
    "        mask = np.logical_and(mask, points[0, :] > 1)\n",
    "        mask = np.logical_and(mask, points[0, :] < cam_img.shape[1] - 1)#cv image has x axis on dimension 1\n",
    "        mask = np.logical_and(mask, points[1, :] > 1)\n",
    "        mask = np.logical_and(mask, points[1, :] < cam_img.shape[0] - 1)\n",
    "        points = points[:, mask]\n",
    "\n",
    "        color_img = np.copy(depth)\n",
    "        color_img = color_img[mask]\n",
    "\n",
    "        color = np.zeros(points.shape).transpose()\n",
    "        scan = scan[:, mask]\n",
    "        for i in range(color.shape[0]):\n",
    "            color[i] = [255, 0, 0] if scan[8, i] > 0 else [0, 0, 255]\n",
    "\n",
    "        #Draw Radar points in image - Red for moving towards ego, blue for moving away from ego\n",
    "        for i in range(points.shape[1]):\n",
    "            p_loc = (int(np.rint(points[0, i])), int(np.rint(points[1, i])))\n",
    "            #text = \"vX: \" + \"{:.5f}\".format(scan[8, i]) + \", vY: \" + \"{:.5f}\".format(scan[9, i])\n",
    "            cv2.circle(cam_img, p_loc, 4, color[i], -1)\n",
    "            #cv2.putText(cam_img, text, p_loc, cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0),thickness=1)\n",
    "        # i = int(points.shape[1] / 4)\n",
    "        # p_loc = (int(np.rint(points[0, i])), int(np.rint(points[1, i])))\n",
    "        # text = \"vX: \" + \"{:.5f}\".format(scan[8, i]) + \", vY: \" + \"{:.5f}\".format(scan[9, i])\n",
    "        # cv2.circle(cam_img, p_loc, 4, color[i], -1)\n",
    "        # cv2.putText(cam_img, text, p_loc, cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0),thickness=1)\n",
    "\n",
    "        cv2.imshow(winname, cam_img)\n",
    "        key = cv2.waitKey(400)\n",
    "        if key == 32:\n",
    "            key = 0\n",
    "            while (key != 32 and key != ord('q') and key != 27): #Space bar\n",
    "                key = cv2.waitKey(0)\n",
    "\n",
    "        if key == ord('q') or key == 27:\n",
    "            break\n",
    "\n",
    "        sample = nusc.get('sample', sample['next'])\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End RADAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize_lidar_in_img(scene, cam_channel='CAM_FRONT')\n",
    "#colorise_pcd(scene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show the map of the scene\n",
    "#view_scene_map(scene)\n",
    "#visualize_lidar_in_img(scene, cam_channel='CAM_FRONT')\n",
    "#colorise_pcd(scene)\n",
    "#cam_vis(scene, \"CAM_BACK\")\n",
    "#interactive_vis(scene, record=True)\n",
    "#top_down(scene, record=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show front cam view through scene\n",
    "#cam_vis(scene, \"CAM_BACK\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Pointcloud LIDAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize with interactive view control\n",
    "#interactive_vis(scene, record=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize top down view only\n",
    "#top_down(scene, record=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuse colored pointcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sample = nusc.get('sample', scene['first_sample_token'])\n",
    "last_sample = nusc.get('sample', scene['last_sample_token'])\n",
    "sample = first_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sample = nusc.get('sample', scene['first_sample_token'])\n",
    "last_sample = nusc.get('sample', scene['last_sample_token'])\n",
    "sample = first_sample\n",
    "channels = ['CAM_FRONT', 'CAM_FRONT_LEFT', 'CAM_FRONT_RIGHT', 'CAM_BACK', 'CAM_BACK_LEFT', 'CAM_BACK_RIGHT']\n",
    "\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "#Visualize a video sequence of LIDAR data  \n",
    "\n",
    "#Render callback for non blocking visualization which allows smoother control of view\n",
    "#Define the callback here to simplify the transfer of variables samples, and pcd\n",
    "def render_callback(vis):\n",
    "    global sample, pcd, channels\n",
    "    cam_channel = 'CAM_FRONT_LEFT'\n",
    "    if sample['token'] == last_sample['token']:\n",
    "        return False\n",
    "\n",
    "    #Load next scan\n",
    "    #cam = nusc.get('sample_data', sample['data'][cam_channel])\n",
    "    #Load lidar scan\n",
    "    lidar = nusc.get('sample_data', sample['data']['LIDAR_TOP'])\n",
    "\n",
    "    #Merged points\n",
    "    pcd_points =np.empty((0, 3))\n",
    "    pcd_colors =np.empty((0, 3))\n",
    "    for channel in channels:\n",
    "        #Load next scan\n",
    "        cam = nusc.get('sample_data', sample['data'][channel])\n",
    "        points, color = project_points_in_img(cam, lidar)\n",
    "        pcd_points = np.append(pcd_points, points, axis=0)\n",
    "        pcd_colors = np.append(pcd_colors, color, axis=0)\n",
    "\n",
    "    #pcd_points, pcd_color = project_points_in_img(cam, lidar)\n",
    "    #####\n",
    "    print(\"FINAL POINTS: \", pcd_points.shape)\n",
    "    lidar_file = nusc.get_sample_data_path(lidar['token'])\n",
    "    scan = np.fromfile(lidar_file, dtype=np.float32).reshape(-1, 5) #x, y, z, intensity, Ring Index\n",
    "    points = scan[:, :3]\n",
    "    #print(\"ORIG Points: \", points.shape)\n",
    "    #print(\"Percentage: \", pcd_points.shape[0] / points.shape[0])\n",
    "    #####\n",
    "    #Update Geometry\n",
    "    if sample['token'] == first_sample['token']:\n",
    "        pcd.points = o3d.utility.Vector3dVector(pcd_points)\n",
    "        pcd.colors = o3d.utility.Vector3dVector(pcd_colors)\n",
    "        vis.add_geometry(pcd)\n",
    "    pcd.points = o3d.utility.Vector3dVector(pcd_points) #Only need XYZ\n",
    "    pcd.colors = o3d.utility.Vector3dVector(pcd_colors)\n",
    "    #vis.clear_geometries()\n",
    "    #vis.add_geometry(pcd)\n",
    "\n",
    "    # Update the visualization window and process events to handle keyboard inputs.\n",
    "    vis.update_geometry(pcd)\n",
    "    vis.poll_events()\n",
    "    vis.update_renderer()\n",
    "\n",
    "    #Next Sample\n",
    "    sample = nusc.get('sample', sample['next'])\n",
    "\n",
    "    #Slow down animation\n",
    "    time.sleep(0.25)\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "# Create an Open3D visualization window.\n",
    "vis = o3d.visualization.VisualizerWithKeyCallback()\n",
    "\n",
    "# Set the custom update function.\n",
    "vis.register_animation_callback(render_callback)\n",
    "\n",
    "\n",
    "# Start animation.\n",
    "vis.create_window()\n",
    "\n",
    "opt = vis.get_render_option()\n",
    "opt.background_color = np.asarray([0, 0, 0])\n",
    "opt.point_size = 2.5 #Original 5\n",
    "\n",
    "vis.run()\n",
    "vis.destroy_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcd_points =np.empty((3, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.append(pcd_points, np.array([[1, 1], [2, 2], [3, 3] ]), axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIDAR 2 IMG Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load lidar and img data from scan\n",
    "sample = first_sample\n",
    "cam_channel = 'CAM_FRONT_RIGHT'\n",
    "#Load camera image\n",
    "cam = nusc.get('sample_data', sample['data'][cam_channel])\n",
    "cam_file = os.path.join(nusc.dataroot, cam['filename'])\n",
    "cam_img = cv2.imread(cam_file)\n",
    "\n",
    "#Load lidar scan\n",
    "lidar = nusc.get('sample_data', sample['data']['LIDAR_TOP'])\n",
    "lidar_file = nusc.get_sample_data_path(lidar['token'])\n",
    "scan = np.fromfile(lidar_file, dtype=np.float32).reshape(-1, 5) #x, y, z, intensity, Ring Index\n",
    "points = scan[:, :3] #Only x, y, z\n",
    "\n",
    "#Get Lidar Pose\n",
    "lidar_sensor = nusc.get('calibrated_sensor', lidar['calibrated_sensor_token'])\n",
    "lidar_t = np.asarray(lidar_sensor['translation'])\n",
    "lidar_r_quat = Quaternion(lidar_sensor['rotation'])\n",
    "\n",
    "#Get Camera Pose and intrinsic parameters\n",
    "cam_sensor = nusc.get('calibrated_sensor', cam['calibrated_sensor_token'])\n",
    "cam_t = np.asarray(cam_sensor['translation'])\n",
    "cam_r_quat = Quaternion(cam_sensor['rotation'])\n",
    "cam_k = np.asarray(cam_sensor['camera_intrinsic'])\n",
    "\n",
    "#Transform points to image frame - Using Open3D transformations\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(points)\n",
    "\n",
    "#First transformation\n",
    "pcd.rotate(lidar_r_quat.rotation_matrix, np.array([0, 0, 0]))\n",
    "pcd.translate(lidar_t)\n",
    "\n",
    "#Second transformation\n",
    "pcd.translate(-cam_t)\n",
    "pcd.rotate(cam_r_quat.rotation_matrix.T, np.array([0, 0, 0]))\n",
    "\n",
    "#Project points into image plane\n",
    "view = np.copy(cam_k)\n",
    "points = np.asarray(pcd.points).T[:3, :]\n",
    "\n",
    "#Set color to depth value\n",
    "depth = points[2, :]\n",
    "\n",
    "#Prepare intrinsics matrix for homogenous transformation\n",
    "viewpad = np.eye(4)\n",
    "viewpad[:view.shape[0], :view.shape[1]] = view\n",
    "\n",
    "#Prepare points for homogenous transformation\n",
    "points = np.concatenate((points, np.ones((1, points.shape[1])))) #4 X N matrix - Homogenous\n",
    "\n",
    "#Project Points into image\n",
    "points = np.dot(viewpad, points)\n",
    "\n",
    "#Remove w\n",
    "points = points[:3, :]\n",
    "\n",
    "#Normalize along Z axis - Divide by depth\n",
    "points = points / points[2, :]\n",
    "\n",
    "# Remove points that are either outside or behind the camera. Leave a margin of 1 pixel for aesthetic reasons.\n",
    "# Also make sure points are at least 1m in front of the camera to avoid seeing the lidar points on the camera\n",
    "# casing for non-keyframes which are slightly out of sync.\n",
    "min_dist = 1.0\n",
    "mask = np.ones(points.shape[1], dtype=bool)\n",
    "mask = np.logical_and(mask, depth > min_dist)\n",
    "mask = np.logical_and(mask, points[0, :] > 1)\n",
    "mask = np.logical_and(mask, points[0, :] < cam_img.shape[1] - 1)#cv image has x axis on dimension 1\n",
    "mask = np.logical_and(mask, points[1, :] > 1)\n",
    "mask = np.logical_and(mask, points[1, :] < cam_img.shape[0] - 1)\n",
    "points_img = np.uint8(np.rint(points))[:2, mask] #The x, y coordinates of the points that lie in the image\n",
    "points = points[:, mask]\n",
    "\n",
    "#Extract color values from image\n",
    "pts_color = np.zeros((points.shape[1], 3))\n",
    "for i in range(points.shape[1]):\n",
    "    pts_color[i] = cam_img[int(np.rint(points[1, i])), int(np.rint(points[0, i]))]\n",
    "#pts_color = (pts_color - pts_color.min()) / (pts_color.max() - pts_color.min()) #Normalize to range 0 to 1\n",
    "\n",
    "#BGR to RGB\n",
    "pts_color = pts_color[:, ::-1]\n",
    "pts_color = pts_color.astype(np.float32) / 255.0\n",
    "\n",
    "pcd_points = scan[mask, :3]\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(pcd_points) #Nx3 #pcd_points\n",
    "pcd.colors = o3d.utility.Vector3dVector(pts_color) #Nx3\n",
    "\n",
    "#o3d.visualization.draw_geometries([pcd])\n",
    "custom_draw_geometry(pcd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcd_points = scan[mask, :3]\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(pcd_points) #Nx3 #pcd_points\n",
    "pcd.colors = o3d.utility.Vector3dVector(pts_color) #Nx3\n",
    "\n",
    "#o3d.visualization.draw_geometries([pcd])\n",
    "custom_draw_geometry(pcd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "winname = cam['channel']\n",
    "window_width, window_height = 800, 600\n",
    "cv2.namedWindow(winname, cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow(winname, window_width, window_height)\n",
    "\n",
    "cv2.imshow(winname, cam_img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite('output_image.jpg', cam_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fusion1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
